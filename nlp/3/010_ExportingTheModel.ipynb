{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Exporting the Model\n",
    "In this notebook, you'll explore options for exporting a BERT checkpoint trained using PyTorch, to NVIDIA Triton Inference Server.\n",
    "\n",
    "**[1.1 Overview: Optimization and Performance](#1.1-Overview:-Optimization-and-Performance)<br>**\n",
    "**[1.2 Export a BERT Checkpoint](#1.2-Export-a-BERT-Checkpoint)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.1 Triton Model Repository](#1.2.1-Triton-Model-Repository)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.2 TorchScript Export](#1.2.2-TorchScript-Export)<br>\n",
    "**[1.3 Test Our Export](#1.3-Test-Our-Export)<br>**\n",
    "**[1.4 Beyond TorchScript](#1.4-Beyond-TorchScript)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.4.1 Exercise: Enable TensorRT Optimization](#1.4.1-Exercise:-Enable-TensorRT-Optimization)<br>\n",
    "**[1.5 Performance Comparison](#1.5-Performance-Comparison)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Overview: Optimization and Performance\n",
    "Optimization of the trained model will have a fairly dramatic impact on the inference performance, measured in bandwidth and latency. Even if the project requirements do not justify investing engineering effort into advanced techniques, such as knowledge distillation or pruning, a fair amount of model performance improvement can be achieved by using model optimization tools. The diagram below illustrates the difference in inference performance between a model deployed using non-optimized TensorFlow, the same model post-processed with TensorRT, and a model fully optimized with TensorRT. \n",
    "\n",
    "<img src=\"images/TFvTRT.jpg\" alt=\"Header\" style=\"width: 600px;\"/>\n",
    "\n",
    "Modern inference servers typically support substantially more than one model format to cater to a wider range of projects, tools, and preferences. Since in this class we are working with a BERT checkpoint trained using PyTorch, and we are deploying it with Triton Inference Server, we will focus on options for deploying PyTorch-based models. These include:\n",
    "   - PyTorch JIT / TorchScript\n",
    "   - ONNX runtime\n",
    "   - ONNX-TensorRT\n",
    "   - TensorRT\n",
    "    \n",
    "It's important to point out that Triton Server supports a much broader set of deployment mechanisms including:\n",
    "   - TensorFlow GraphDef\n",
    "   - TensorFlow saved model\n",
    "   - Caffe 2 exports\n",
    "   - Custom models (which can be any custom executable)\n",
    "\n",
    "In this section we will look at how to deploy a model using some of the deployment engines listed above and the impact each has on performance. We will also experiment with some of the key settings, namely the batch size and numerical precision (FP32 and FP16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Export a BERT Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model checkpoint we want to deploy, <code>bert_qa.pt</code>, should be located in your `data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bert_qa.pt\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a standard checkpoint of a BERT-Large network, fine-tuned on the [Stanford Question Answering Dataset (SQuAD)](https://arxiv.org/abs/1606.05250). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Scripts\n",
    "As we explore various deployment configurations, we'll repeat some steps over and over.  Therefore, we'll use some helper scripts to partially automate the process so that we can focus our attention on the configuration settings and results.  You can explore the code details yourself if you are curious:\n",
    "\n",
    "- [utilities/wait_for_triton_server.sh](utilities/wait_for_triton_server.sh): Check the \"live\" and \"ready\" status of the Triton server via the API\n",
    "- [deployer/deployer.py](deployer/deployer.py): Convert a checkpoint to a deployable model and export it\n",
    "- [utilities/run_perf_analyzer_local.sh](utilities/run_perf_analyzer_local.sh): Measure performance with the [perf_analyzer](https://github.com/triton-inference-server/client/blob/main/src/c++/perf_analyzer/README.md) application\n",
    "- [utilities/run_warmup.sh](utilities/run_warmup.sh): Run some inferences using `perf_analyzer` to warm up the model.  Prewarming the model results in more stable measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Triton server has been deployed in a container and is available to us at host \"triton\" on port \"8000\". Run the next cell to to check for a \"200 OK\" HTTP response from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Triton Model Repository\n",
    "When Triton Server is started, it is typically configured to observe a local or remote file system where models are hosted. The directory which is being observed is called a *model repository*. A typical command to start the Triton Server identifies the location of the model repository with an option:<br>\n",
    "```bash\n",
    "tritonserver --model-repository=\"/path/to/model/repository\"\n",
    "```\n",
    "\n",
    "The model repository needs to have the following layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<model-repository-path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab container is configured to use the <code>./model_repository</code> folder as the model repository, so any change within this folder will affect the behavior of Triton Server.<br/>\n",
    "\n",
    "In order to expose a new model to Triton you need to: <br/>\n",
    "   1. Create a new model folder in the model repository. The name of the folder needs to reflect the name of the service you will be exposing to your users/applications.<br/>\n",
    "   2. Within the model folder, create a <code>config.pbtxt</code> file that contains the basic serving configuration for the model<br/>\n",
    "   3. Also within the model folder, create at least one folder containing a copy of the model. The name of the folder reflects the version name of the model. You can create and host multiple versions of the same model.<br/>\n",
    "    \n",
    "Next, we'll walk through the process of exporting the model to Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 TorchScript Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab we will:\n",
    "   - Convert the PyTorch checkpoint into [TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript)\n",
    "   - Generate the Triton configuration file\n",
    "   - Deploy the created assets to our model repository\n",
    "Please execute the cells below. Since we are loading a PyTorch checkpoint and converting it into TorchScript, it might take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-torchscript in format pytorch_libtorch\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_recursive.py:260: UserWarning: 'bias' was found in ScriptModule constants,  but it is a non-constant parameter. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.02516031265258789\n",
      "\n",
      "average L_inf error over output tensors:  0.013570129871368408\n",
      "variance of L_inf error over output tensors:  0.00011137690313015962\n",
      "stddev of L_inf error over output tensors:  0.010553525625598283\n",
      "\n",
      "time of error check of native model:  0.7649025917053223 seconds\n",
      "time of error check of ts model:  1.422410249710083 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --ts-script \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint \"/dli/task/data/bert_qa.pt\" \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deployer.py` script loads the `bert_qa.pt` checkpoint, deploys it in `ts-script` format into a folder called `bertQA-torchscript`, and marks it as version `1`. We will discuss some of the more advanced settings later. For now, let's inspect the files generated by the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 May 31 19:29 .\n",
      "drwxr-xr-x 3 root root 4096 May 31 19:29 ..\n",
      "drwxr-xr-x 2 root root 4096 May 31 19:29 1\n",
      "-rw-r--r-- 1 root root  568 May 31 19:29 config.pbtxt\n",
      "total 1309296\n",
      "drwxr-xr-x 2 root root       4096 May 31 19:29 .\n",
      "drwxr-xr-x 3 root root       4096 May 31 19:29 ..\n",
      "-rw-r--r-- 1 root root 1340709950 May 31 19:29 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-torchscript/\n",
    "!ls -al ./candidatemodels/bertQA-torchscript/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the script exported the model into the TorchScript format and saved it as `model.pt`. It also generated the `config.pbtxt` file. <br> \n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"bertQA-torchscript\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 8\n",
      "input [\n",
      "{\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__2\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "output [\n",
      "{\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}, \n",
      "{\n",
      "    name: \"output__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: 0\n",
      "  }\n",
      "}\n",
      "instance_group [\n",
      "    {\n",
      "        count: 1\n",
      "        kind: KIND_GPU\n",
      "        gpus: [ 0 ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat ./candidatemodels/bertQA-torchscript/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is fairly simple and defines:\n",
    "   - Name of the model\n",
    "   - Type of platform to be used for inference; in this case `pytorch_libtorch`\n",
    "   - Input and output dimensions used by the network\n",
    "   - Optimizations used; in this case GPU and the default TorchScript optimization \n",
    "   - Instance group configuration; in this case instance group count is set to one, meaning that only one copy of the model will be held in GPU memory (GPU 0 is being used).\n",
    "    \n",
    "To deploy the model, move the folder to the Triton model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-torchscript model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You have successfully deployed your first model to Triton Inference Server!\n",
    "\n",
    "We'll come back to discuss the detailed configuration later, but for now let's see how our model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.3 Test Our Export\n",
    "Execute the cells below to start an inference process and make a simple measurement of inference performance. First, we'll set up some configuration. `maxConcurrency` is set to two, meaning that the stress test will be executed twice. The first run will use just a single thread and the second one will use two threads to query the server. Without turning on the concurrent model execution or dynamic batching features, what do you think will be the impact on performance of running two processes querying the server? Do you think:<br/>\n",
    "- Bandwidth will increase or decrease?<br/>\n",
    "- Latency will increase or decrease?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelVersion = \"1\"\n",
    "precision = \"fp32\"\n",
    "batchSize = \"8\"\n",
    "maxLatency = \"500\"\n",
    "maxClientThreads = \"10\"\n",
    "maxConcurrency = \"2\"\n",
    "dockerBridge = \"host\"\n",
    "resultsFolderName = \"1\"\n",
    "profilingData = \"utilities/profiling_data_int64\"\n",
    "measurement_request_count = 50\n",
    "percentile_stability = 85\n",
    "stability_percentage = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-torchscript\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 2 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 34.6628 infer/sec. p85 latency: 141973 usec\n",
      "  Pass [2] throughput: 55.9921 infer/sec. p85 latency: 142200 usec\n",
      "  Pass [3] throughput: 55.9925 infer/sec. p85 latency: 143030 usec\n",
      "  Pass [4] throughput: 55.9924 infer/sec. p85 latency: 143572 usec\n",
      "  Client: \n",
      "    Request count: 168\n",
      "    Throughput: 55.9923 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 7.00 infer/sec\n",
      "    p50 latency: 142569 usec\n",
      "    p85 latency: 143101 usec\n",
      "    p90 latency: 143149 usec\n",
      "    p95 latency: 143572 usec\n",
      "    p99 latency: 143769 usec\n",
      "    Avg gRPC time: 142523 usec (marshal 5 usec + response wait 142517 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1344\n",
      "    Execution count: 168\n",
      "    Successful request count: 168\n",
      "    Avg request latency: 142302 usec (overhead 84 usec + queue 25 usec + compute input 45 usec + compute infer 142107 usec + compute output 40 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 55.9932 infer/sec. p85 latency: 287641 usec\n",
      "  Pass [2] throughput: 55.9929 infer/sec. p85 latency: 288859 usec\n",
      "  Pass [3] throughput: 54.9918 infer/sec. p85 latency: 289446 usec\n",
      "  Client: \n",
      "    Request count: 167\n",
      "    Throughput: 55.6593 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 7.00 infer/sec\n",
      "    p50 latency: 287678 usec\n",
      "    p85 latency: 289127 usec\n",
      "    p90 latency: 289338 usec\n",
      "    p95 latency: 289457 usec\n",
      "    p99 latency: 289554 usec\n",
      "    Avg gRPC time: 286342 usec (marshal 6 usec + response wait 286335 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1336\n",
      "    Execution count: 167\n",
      "    Successful request count: 167\n",
      "    Avg request latency: 286052 usec (overhead 83 usec + queue 142094 usec + compute input 44 usec + compute infer 143789 usec + compute output 40 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 55.9923 infer/sec, latency 143101 usec\n",
      "Concurrency: 2, throughput: 55.6593 infer/sec, latency 289127 usec\n",
      "CPU times: user 541 ms, sys: 185 ms, total: 727 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-torchscript\"\n",
    "maxConcurrency = \"2\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went okay you should have been presented with output similar to the following example result, showing the inference performance across two different configurations.<br/>\n",
    "<img src=\"images/InferenceJob1.png\" alt=\"Example output of inference job 1\" style=\"width: 1200px;\"/>\n",
    "\n",
    "If you happened to get \"error: failed to get model metatdata\", try running the cell again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Beyond TorchScript\n",
    "\n",
    "Let's investigate a different route for model deployment onto Triton, namely <a href=\"https://onnx.ai\">Open Neural Network Exchange (ONNX)</a>. ONNX is an open format for representation and exchange of neural network models. It defines a common set of operators that are used to build common models, as well as a file format for exchanging them. The advantage of ONNX is that it is relatively widely adopted and can be used to exchange models between <a href=\"https://onnx.ai/supported-tools.html\">a wide range of deep learning tools</a>, such as deep learning frameworks or deployment tools. This also includes TensorRT, which can consume ONNX models. </br>\n",
    "\n",
    "As before, start by exporting the model, but this time using the ONNX format. We will take advantage of the export tool that we used earlier, but change the export format from <code>ts-script</code> to <code>onnx</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.32570981979370117 seconds\n",
      "time of error check of onnx model:  10.325327157974243 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md\">TorchScript serialization format</a>, the <a href=\"https://onnx.ai/get-started.html\">ONNX format</a> can be inspected quite easily (and parts are human readable). Lets have a look at the assets our export has generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 May 31 19:31 .\n",
      "drwxr-xr-x 3 root root 4096 May 31 19:31 ..\n",
      "drwxr-xr-x 2 root root 4096 May 31 19:31 1\n",
      "-rw-r--r-- 1 root root  561 May 31 19:31 config.pbtxt\n",
      "total 1305596\n",
      "drwxr-xr-x 2 root root       4096 May 31 19:31 .\n",
      "drwxr-xr-x 3 root root       4096 May 31 19:31 ..\n",
      "-rw-r--r-- 1 root root 1336922015 May 31 19:31 model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx/\n",
    "!ls -al ./candidatemodels/bertQA-onnx/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have a configuration file as well as a model, this time stored in ONNX format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for executing the ONNX-based export in Triton:\n",
    "- We can take advantage of ONNX runtime </br>\n",
    "- We can ask TensorRT to parse the ONNX assets in order to generate a TensorRT engine to use instead </br>\n",
    "\n",
    "We'll try both approaches and look at the impact this has on inference performance. In order to deploy the current ONNX model, move it to the model repository..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run our stress testing code across 10 different levels of concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 67.9906 infer/sec. p85 latency: 117441 usec\n",
      "  Pass [2] throughput: 67.9899 infer/sec. p85 latency: 117337 usec\n",
      "  Pass [3] throughput: 67.9891 infer/sec. p85 latency: 117346 usec\n",
      "  Client: \n",
      "    Request count: 153\n",
      "    Throughput: 67.9898 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.55 infer/sec\n",
      "    p50 latency: 117295 usec\n",
      "    p85 latency: 117370 usec\n",
      "    p90 latency: 117405 usec\n",
      "    p95 latency: 117441 usec\n",
      "    p99 latency: 117659 usec\n",
      "    Avg gRPC time: 117307 usec (marshal 5 usec + response wait 117301 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1224\n",
      "    Execution count: 153\n",
      "    Successful request count: 153\n",
      "    Avg request latency: 117091 usec (overhead 70 usec + queue 25 usec + compute input 30 usec + compute infer 116954 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 67.9904 infer/sec. p85 latency: 234229 usec\n",
      "  Pass [2] throughput: 67.9887 infer/sec. p85 latency: 234083 usec\n",
      "  Pass [3] throughput: 69.321 infer/sec. p85 latency: 234028 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4334 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 234033 usec\n",
      "    p85 latency: 234156 usec\n",
      "    p90 latency: 234171 usec\n",
      "    p95 latency: 234227 usec\n",
      "    p99 latency: 234253 usec\n",
      "    Avg gRPC time: 232926 usec (marshal 5 usec + response wait 232920 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 232676 usec (overhead 67 usec + queue 115667 usec + compute input 30 usec + compute infer 116900 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 67.9893 infer/sec. p85 latency: 351128 usec\n",
      "  Pass [2] throughput: 67.9884 infer/sec. p85 latency: 351091 usec\n",
      "  Pass [3] throughput: 69.3198 infer/sec. p85 latency: 350968 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4325 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 350947 usec\n",
      "    p85 latency: 351060 usec\n",
      "    p90 latency: 351099 usec\n",
      "    p95 latency: 351148 usec\n",
      "    p99 latency: 351208 usec\n",
      "    Avg gRPC time: 349199 usec (marshal 5 usec + response wait 349193 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 348899 usec (overhead 67 usec + queue 231922 usec + compute input 27 usec + compute infer 116872 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 67.99 infer/sec. p85 latency: 467824 usec\n",
      "  Pass [2] throughput: 67.988 infer/sec. p85 latency: 467937 usec\n",
      "  Pass [3] throughput: 69.3221 infer/sec. p85 latency: 467868 usec\n",
      "  Client: \n",
      "    Request count: 154\n",
      "    Throughput: 68.4334 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 8.61 infer/sec\n",
      "    p50 latency: 467801 usec\n",
      "    p85 latency: 467881 usec\n",
      "    p90 latency: 467919 usec\n",
      "    p95 latency: 467961 usec\n",
      "    p99 latency: 468017 usec\n",
      "    Avg gRPC time: 465355 usec (marshal 6 usec + response wait 465348 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1232\n",
      "    Execution count: 154\n",
      "    Successful request count: 154\n",
      "    Avg request latency: 465060 usec (overhead 65 usec + queue 348121 usec + compute input 26 usec + compute infer 116836 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 67.9906 infer/sec. p85 latency: 585029 usec\n",
      "  Pass [2] throughput: 67.9891 infer/sec. p85 latency: 585099 usec\n",
      "  Pass [3] throughput: 69.3219 infer/sec. p85 latency: 584910 usec\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 67.9898 infer/sec, latency 117370 usec\n",
      "Concurrency: 2, throughput: 68.4334 infer/sec, latency 234156 usec\n",
      "Concurrency: 3, throughput: 68.4325 infer/sec, latency 351060 usec\n",
      "Concurrency: 4, throughput: 68.4334 infer/sec, latency 467881 usec\n",
      "CPU times: user 678 ms, sys: 284 ms, total: 962 ms\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the results. Did we manage to run our benchmark at all 10 concurrency levels (or did the benchmark time out earlier)? What happened to the request latency in relation to the 500 ms time limit we configured?</br>\n",
    "\n",
    "Now let's export the ONNX model again, so that we can configure it for TensorRT execution.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-fp16 in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.3260340690612793 seconds\n",
      "time of error check of onnx model:  10.415302991867065 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the above command should have generated the ONNX export as well as a configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 May 31 19:34 .\n",
      "drwxr-xr-x 3 root root 4096 May 31 19:33 ..\n",
      "drwxr-xr-x 2 root root 4096 May 31 19:34 1\n",
      "-rw-r--r-- 1 root root  570 May 31 19:34 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx-trt-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 Exercise: Enable TensorRT Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to enable TensorRT, we need to add an additional section to the \"config.pbtxt\" configuration file. In particular, we need to add an additional segment to the <code>optimization</code> section:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt) to enable TensorRT. Feel free to look at the [solution](solutions/ex-1-4-1_config.pbtxt) as needed.\n",
    "2. Once you have saved your changes (Main menu: File -> Save File), move the folder to the model repository using the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "!cp solutions/ex-1-4-1_config.pbtxt candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: inter-device move failed: './candidatemodels/bertQA-onnx-trt-fp16' to 'model_repository/bertQA-onnx-trt-fp16'; unable to remove target: Directory not empty\n"
     ]
    }
   ],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-fp16 model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Execute our profiling tool in the next cell and investigate the impact on performance. This could take a while to start, as we are waiting for the server to migrate the model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 6.32033 infer/sec. p85 latency: 40818 usec\n",
      "  Pass [2] throughput: 199.935 infer/sec. p85 latency: 40771 usec\n",
      "  Pass [3] throughput: 194.62 infer/sec. p85 latency: 40753 usec\n",
      "  Pass [4] throughput: 199.94 infer/sec. p85 latency: 40739 usec\n",
      "  Client: \n",
      "    Request count: 173\n",
      "    Throughput: 197.659 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 24.77 infer/sec\n",
      "    p50 latency: 40688 usec\n",
      "    p85 latency: 40752 usec\n",
      "    p90 latency: 40779 usec\n",
      "    p95 latency: 40799 usec\n",
      "    p99 latency: 40861 usec\n",
      "    Avg gRPC time: 40687 usec (marshal 5 usec + response wait 40681 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1384\n",
      "    Execution count: 173\n",
      "    Successful request count: 173\n",
      "    Avg request latency: 40442 usec (overhead 68 usec + queue 26 usec + compute input 30 usec + compute infer 40307 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 197.295 infer/sec. p85 latency: 80883 usec\n",
      "  Pass [2] throughput: 197.286 infer/sec. p85 latency: 80877 usec\n",
      "  Pass [3] throughput: 197.279 infer/sec. p85 latency: 80873 usec\n",
      "  Client: \n",
      "    Request count: 222\n",
      "    Throughput: 197.287 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.77 infer/sec\n",
      "    p50 latency: 80833 usec\n",
      "    p85 latency: 80880 usec\n",
      "    p90 latency: 80889 usec\n",
      "    p95 latency: 80911 usec\n",
      "    p99 latency: 80931 usec\n",
      "    Avg gRPC time: 80642 usec (marshal 5 usec + response wait 80636 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1776\n",
      "    Execution count: 222\n",
      "    Successful request count: 222\n",
      "    Avg request latency: 80377 usec (overhead 66 usec + queue 39972 usec + compute input 29 usec + compute infer 40300 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 199.95 infer/sec. p85 latency: 121314 usec\n",
      "  Pass [2] throughput: 197.288 infer/sec. p85 latency: 121301 usec\n",
      "  Pass [3] throughput: 199.921 infer/sec. p85 latency: 121301 usec\n",
      "  Client: \n",
      "    Request count: 174\n",
      "    Throughput: 198.801 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 25.05 infer/sec\n",
      "    p50 latency: 121244 usec\n",
      "    p85 latency: 121303 usec\n",
      "    p90 latency: 121314 usec\n",
      "    p95 latency: 121338 usec\n",
      "    p99 latency: 121369 usec\n",
      "    Avg gRPC time: 120593 usec (marshal 5 usec + response wait 120587 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1392\n",
      "    Execution count: 174\n",
      "    Successful request count: 174\n",
      "    Avg request latency: 120280 usec (overhead 67 usec + queue 79878 usec + compute input 29 usec + compute infer 40296 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 197.29 infer/sec. p85 latency: 161693 usec\n",
      "  Pass [2] throughput: 197.283 infer/sec. p85 latency: 161725 usec\n",
      "  Pass [3] throughput: 199.945 infer/sec. p85 latency: 161695 usec\n",
      "  Client: \n",
      "    Request count: 198\n",
      "    Throughput: 197.951 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.88 infer/sec\n",
      "    p50 latency: 161652 usec\n",
      "    p85 latency: 161699 usec\n",
      "    p90 latency: 161725 usec\n",
      "    p95 latency: 161750 usec\n",
      "    p99 latency: 161842 usec\n",
      "    Avg gRPC time: 161026 usec (marshal 6 usec + response wait 161019 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1584\n",
      "    Execution count: 198\n",
      "    Successful request count: 198\n",
      "    Avg request latency: 160727 usec (overhead 67 usec + queue 120326 usec + compute input 29 usec + compute infer 40295 usec + compute output 9 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 197.293 infer/sec. p85 latency: 202113 usec\n",
      "  Pass [2] throughput: 197.275 infer/sec. p85 latency: 202554 usec\n",
      "  Pass [3] throughput: 197.293 infer/sec. p85 latency: 202213 usec\n",
      "  Client: \n",
      "    Request count: 222\n",
      "    Throughput: 197.287 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.77 infer/sec\n",
      "    p50 latency: 202126 usec\n",
      "    p85 latency: 202234 usec\n",
      "    p90 latency: 202275 usec\n",
      "    p95 latency: 202554 usec\n",
      "    p99 latency: 203059 usec\n",
      "    Avg gRPC time: 201419 usec (marshal 7 usec + response wait 201411 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1776\n",
      "    Execution count: 222\n",
      "    Successful request count: 222\n",
      "    Avg request latency: 201108 usec (overhead 76 usec + queue 160688 usec + compute input 31 usec + compute infer 40301 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 199.958 infer/sec. p85 latency: 242611 usec\n",
      "  Pass [2] throughput: 197.296 infer/sec. p85 latency: 242649 usec\n",
      "  Pass [3] throughput: 197.295 infer/sec. p85 latency: 242537 usec\n",
      "  Client: \n",
      "    Request count: 198\n",
      "    Throughput: 197.961 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.94 infer/sec\n",
      "    p50 latency: 242533 usec\n",
      "    p85 latency: 242611 usec\n",
      "    p90 latency: 242627 usec\n",
      "    p95 latency: 242659 usec\n",
      "    p99 latency: 242706 usec\n",
      "    Avg gRPC time: 241367 usec (marshal 7 usec + response wait 241359 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1584\n",
      "    Execution count: 198\n",
      "    Successful request count: 198\n",
      "    Avg request latency: 241068 usec (overhead 72 usec + queue 200658 usec + compute input 30 usec + compute infer 40297 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 199.954 infer/sec. p85 latency: 282985 usec\n",
      "  Pass [2] throughput: 197.281 infer/sec. p85 latency: 283008 usec\n",
      "  Pass [3] throughput: 197.286 infer/sec. p85 latency: 283060 usec\n",
      "  Client: \n",
      "    Request count: 198\n",
      "    Throughput: 197.951 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.94 infer/sec\n",
      "    p50 latency: 282945 usec\n",
      "    p85 latency: 283024 usec\n",
      "    p90 latency: 283045 usec\n",
      "    p95 latency: 283069 usec\n",
      "    p99 latency: 283115 usec\n",
      "    Avg gRPC time: 281581 usec (marshal 7 usec + response wait 281573 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1584\n",
      "    Execution count: 198\n",
      "    Successful request count: 198\n",
      "    Avg request latency: 281288 usec (overhead 69 usec + queue 240880 usec + compute input 29 usec + compute infer 40299 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 199.958 infer/sec. p85 latency: 323491 usec\n",
      "  Pass [2] throughput: 197.294 infer/sec. p85 latency: 323490 usec\n",
      "  Pass [3] throughput: 197.285 infer/sec. p85 latency: 323534 usec\n",
      "  Client: \n",
      "    Request count: 198\n",
      "    Throughput: 197.957 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.94 infer/sec\n",
      "    p50 latency: 323409 usec\n",
      "    p85 latency: 323492 usec\n",
      "    p90 latency: 323509 usec\n",
      "    p95 latency: 323540 usec\n",
      "    p99 latency: 323704 usec\n",
      "    Avg gRPC time: 321852 usec (marshal 7 usec + response wait 321844 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1584\n",
      "    Execution count: 198\n",
      "    Successful request count: 198\n",
      "    Avg request latency: 321549 usec (overhead 70 usec + queue 281134 usec + compute input 29 usec + compute infer 40305 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 199.955 infer/sec. p85 latency: 363862 usec\n",
      "  Pass [2] throughput: 197.297 infer/sec. p85 latency: 363930 usec\n",
      "  Pass [3] throughput: 197.292 infer/sec. p85 latency: 363927 usec\n",
      "  Client: \n",
      "    Request count: 198\n",
      "    Throughput: 197.96 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.94 infer/sec\n",
      "    p50 latency: 363809 usec\n",
      "    p85 latency: 363909 usec\n",
      "    p90 latency: 363935 usec\n",
      "    p95 latency: 363964 usec\n",
      "    p99 latency: 364027 usec\n",
      "    Avg gRPC time: 362065 usec (marshal 7 usec + response wait 362057 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1584\n",
      "    Execution count: 198\n",
      "    Successful request count: 198\n",
      "    Avg request latency: 361761 usec (overhead 70 usec + queue 321350 usec + compute input 29 usec + compute infer 40302 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 197.3 infer/sec. p85 latency: 404342 usec\n",
      "  Pass [2] throughput: 199.945 infer/sec. p85 latency: 404278 usec\n",
      "  Pass [3] throughput: 197.291 infer/sec. p85 latency: 404352 usec\n",
      "  Client: \n",
      "    Request count: 198\n",
      "    Throughput: 197.958 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 24.88 infer/sec\n",
      "    p50 latency: 404223 usec\n",
      "    p85 latency: 404340 usec\n",
      "    p90 latency: 404353 usec\n",
      "    p95 latency: 404371 usec\n",
      "    p99 latency: 404410 usec\n",
      "    Avg gRPC time: 402284 usec (marshal 7 usec + response wait 402276 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1584\n",
      "    Execution count: 198\n",
      "    Successful request count: 198\n",
      "    Avg request latency: 401968 usec (overhead 68 usec + queue 361557 usec + compute input 29 usec + compute infer 40303 usec + compute output 10 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 197.659 infer/sec, latency 40752 usec\n",
      "Concurrency: 2, throughput: 197.287 infer/sec, latency 80880 usec\n",
      "Concurrency: 3, throughput: 198.801 infer/sec, latency 121303 usec\n",
      "Concurrency: 4, throughput: 197.951 infer/sec, latency 161699 usec\n",
      "Concurrency: 5, throughput: 197.287 infer/sec, latency 202234 usec\n",
      "Concurrency: 6, throughput: 197.961 infer/sec, latency 242611 usec\n",
      "Concurrency: 7, throughput: 197.951 infer/sec, latency 283024 usec\n",
      "Concurrency: 8, throughput: 197.957 infer/sec, latency 323492 usec\n",
      "Concurrency: 9, throughput: 197.96 infer/sec, latency 363909 usec\n",
      "Concurrency: 10, throughput: 197.958 infer/sec, latency 404340 usec\n",
      "CPU times: user 1.09 s, sys: 593 ms, total: 1.69 s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \" + modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the performance against ONNX runtime. \n",
    "* How did the latency change, especially across larger concurrency runs? \n",
    "* How did the bandwidth change? Can you explain the level of bandwidth change observed? \n",
    "* Why did the ONNX model timeout at concurrency of less than 10? How does the TensorRT latency at concurrency 10 compare to latency of pure ONNX runtime at an earlier concurrency?\n",
    "\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've successfully deployed an NLP model to Triton Server with TorchScript and applied both reduced precision and TensorRT optimizations.\n",
    "In the next notebook you'll learn how to optimize the model itself and to deploy it in an efficient way. \n",
    "\n",
    "Please proceed to the next notebook:<br>\n",
    "[2.0 Hosting the model](020_HostingTheModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
