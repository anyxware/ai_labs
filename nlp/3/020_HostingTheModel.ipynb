{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#scheduling-and-batching\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion = \"1\"\n",
    "precision = \"fp32\"\n",
    "batchSize = \"8\"\n",
    "maxLatency = \"500\"\n",
    "maxClientThreads = \"10\"\n",
    "maxConcurrency = \"2\"\n",
    "dockerBridge = \"host\"\n",
    "resultsFolderName = \"1\"\n",
    "profilingData = \"utilities/profiling_data_int64\"\n",
    "measurement_request_count = 50\n",
    "percentile_stability = 85\n",
    "stability_percentage = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 0.960315 infer/sec. p85 latency: 11248 usec\n",
      "  Pass [2] throughput: 89.9407 infer/sec. p85 latency: 11238 usec\n",
      "  Pass [3] throughput: 88.9463 infer/sec. p85 latency: 11246 usec\n",
      "  Pass [4] throughput: 88.9629 infer/sec. p85 latency: 11229 usec\n",
      "  Client: \n",
      "    Request count: 268\n",
      "    Throughput: 89.2833 infer/sec\n",
      "    Avg client overhead: 0.03%\n",
      "    Avg send request rate: 89.28 infer/sec\n",
      "    p50 latency: 11191 usec\n",
      "    p85 latency: 11237 usec\n",
      "    p90 latency: 11245 usec\n",
      "    p95 latency: 11254 usec\n",
      "    p99 latency: 11295 usec\n",
      "    Avg gRPC time: 11187 usec (marshal 2 usec + response wait 11185 usec + unmarshal 0 usec)\n",
      "  Server: \n",
      "    Inference count: 268\n",
      "    Execution count: 268\n",
      "    Successful request count: 268\n",
      "    Avg request latency: 11059 usec (overhead 67 usec + queue 26 usec + compute input 25 usec + compute infer 10933 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 90.9656 infer/sec. p85 latency: 22114 usec\n",
      "  Pass [2] throughput: 90.9479 infer/sec. p85 latency: 22113 usec\n",
      "  Pass [3] throughput: 89.9637 infer/sec. p85 latency: 22121 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.6258 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 22080 usec\n",
      "    p85 latency: 22114 usec\n",
      "    p90 latency: 22123 usec\n",
      "    p95 latency: 22138 usec\n",
      "    p99 latency: 22157 usec\n",
      "    Avg gRPC time: 21999 usec (marshal 2 usec + response wait 21996 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 21846 usec (overhead 67 usec + queue 10817 usec + compute input 23 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 90.9684 infer/sec. p85 latency: 33213 usec\n",
      "  Pass [2] throughput: 89.943 infer/sec. p85 latency: 33213 usec\n",
      "  Pass [3] throughput: 90.9639 infer/sec. p85 latency: 33166 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.6251 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 33134 usec\n",
      "    p85 latency: 33193 usec\n",
      "    p90 latency: 33216 usec\n",
      "    p95 latency: 33244 usec\n",
      "    p99 latency: 33308 usec\n",
      "    Avg gRPC time: 33016 usec (marshal 3 usec + response wait 33012 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 32816 usec (overhead 66 usec + queue 21783 usec + compute input 22 usec + compute infer 10936 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 90.9674 infer/sec. p85 latency: 44195 usec\n",
      "  Pass [2] throughput: 89.9579 infer/sec. p85 latency: 44213 usec\n",
      "  Pass [3] throughput: 90.963 infer/sec. p85 latency: 44194 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.6294 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 44156 usec\n",
      "    p85 latency: 44195 usec\n",
      "    p90 latency: 44203 usec\n",
      "    p95 latency: 44224 usec\n",
      "    p99 latency: 44243 usec\n",
      "    Avg gRPC time: 44008 usec (marshal 3 usec + response wait 44004 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 43808 usec (overhead 66 usec + queue 32780 usec + compute input 22 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 89.9732 infer/sec. p85 latency: 55228 usec\n",
      "  Pass [2] throughput: 90.9632 infer/sec. p85 latency: 55245 usec\n",
      "  Pass [3] throughput: 90.9636 infer/sec. p85 latency: 55243 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.6334 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 90.97 infer/sec\n",
      "    p50 latency: 55193 usec\n",
      "    p85 latency: 55234 usec\n",
      "    p90 latency: 55249 usec\n",
      "    p95 latency: 55272 usec\n",
      "    p99 latency: 55304 usec\n",
      "    Avg gRPC time: 55007 usec (marshal 3 usec + response wait 55003 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 54799 usec (overhead 66 usec + queue 43772 usec + compute input 22 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 89.9698 infer/sec. p85 latency: 66291 usec\n",
      "  Pass [2] throughput: 90.964 infer/sec. p85 latency: 66286 usec\n",
      "  Pass [3] throughput: 90.9587 infer/sec. p85 latency: 66282 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.6308 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 66240 usec\n",
      "    p85 latency: 66285 usec\n",
      "    p90 latency: 66297 usec\n",
      "    p95 latency: 66319 usec\n",
      "    p99 latency: 66350 usec\n",
      "    Avg gRPC time: 66017 usec (marshal 3 usec + response wait 66013 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 65818 usec (overhead 67 usec + queue 54790 usec + compute input 22 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 89.9697 infer/sec. p85 latency: 77331 usec\n",
      "  Pass [2] throughput: 90.9636 infer/sec. p85 latency: 77341 usec\n",
      "  Pass [3] throughput: 89.9622 infer/sec. p85 latency: 77343 usec\n",
      "  Client: \n",
      "    Request count: 271\n",
      "    Throughput: 90.2985 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 77282 usec\n",
      "    p85 latency: 77339 usec\n",
      "    p90 latency: 77353 usec\n",
      "    p95 latency: 77377 usec\n",
      "    p99 latency: 77414 usec\n",
      "    Avg gRPC time: 77027 usec (marshal 3 usec + response wait 77023 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 271\n",
      "    Execution count: 271\n",
      "    Successful request count: 271\n",
      "    Avg request latency: 76820 usec (overhead 68 usec + queue 65791 usec + compute input 23 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 89.9692 infer/sec. p85 latency: 88378 usec\n",
      "  Pass [2] throughput: 90.9622 infer/sec. p85 latency: 88423 usec\n",
      "  Pass [3] throughput: 89.9649 infer/sec. p85 latency: 88367 usec\n",
      "  Client: \n",
      "    Request count: 271\n",
      "    Throughput: 90.2988 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 90.63 infer/sec\n",
      "    p50 latency: 88339 usec\n",
      "    p85 latency: 88388 usec\n",
      "    p90 latency: 88404 usec\n",
      "    p95 latency: 88435 usec\n",
      "    p99 latency: 88470 usec\n",
      "    Avg gRPC time: 88046 usec (marshal 3 usec + response wait 88042 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 271\n",
      "    Execution count: 271\n",
      "    Successful request count: 271\n",
      "    Avg request latency: 87832 usec (overhead 68 usec + queue 76802 usec + compute input 23 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 90.9706 infer/sec. p85 latency: 99421 usec\n",
      "  Pass [2] throughput: 90.9646 infer/sec. p85 latency: 99427 usec\n",
      "  Pass [3] throughput: 89.964 infer/sec. p85 latency: 99429 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.6331 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 90.97 infer/sec\n",
      "    p50 latency: 99372 usec\n",
      "    p85 latency: 99427 usec\n",
      "    p90 latency: 99437 usec\n",
      "    p95 latency: 99451 usec\n",
      "    p99 latency: 99476 usec\n",
      "    Avg gRPC time: 99006 usec (marshal 3 usec + response wait 99002 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 98790 usec (overhead 68 usec + queue 87761 usec + compute input 23 usec + compute infer 10931 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 90.9688 infer/sec. p85 latency: 110467 usec\n",
      "  Pass [2] throughput: 89.9533 infer/sec. p85 latency: 110525 usec\n",
      "  Pass [3] throughput: 90.9531 infer/sec. p85 latency: 110555 usec\n",
      "  Client: \n",
      "    Request count: 272\n",
      "    Throughput: 90.625 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 110431 usec\n",
      "    p85 latency: 110503 usec\n",
      "    p90 latency: 110535 usec\n",
      "    p95 latency: 110598 usec\n",
      "    p99 latency: 110728 usec\n",
      "    Avg gRPC time: 110037 usec (marshal 3 usec + response wait 110033 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 272\n",
      "    Execution count: 272\n",
      "    Successful request count: 272\n",
      "    Avg request latency: 109817 usec (overhead 68 usec + queue 98785 usec + compute input 22 usec + compute infer 10933 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 89.2833 infer/sec, latency 11237 usec\n",
      "Concurrency: 2, throughput: 90.6258 infer/sec, latency 22114 usec\n",
      "Concurrency: 3, throughput: 90.6251 infer/sec, latency 33193 usec\n",
      "Concurrency: 4, throughput: 90.6294 infer/sec, latency 44195 usec\n",
      "Concurrency: 5, throughput: 90.6334 infer/sec, latency 55234 usec\n",
      "Concurrency: 6, throughput: 90.6308 infer/sec, latency 66285 usec\n",
      "Concurrency: 7, throughput: 90.2985 infer/sec, latency 77339 usec\n",
      "Concurrency: 8, throughput: 90.2988 infer/sec, latency 88388 usec\n",
      "Concurrency: 9, throughput: 90.6331 infer/sec, latency 99427 usec\n",
      "Concurrency: 10, throughput: 90.625 infer/sec, latency 110503 usec\n",
      "CPU times: user 982 ms, sys: 382 ms, total: 1.36 s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have observed utilization similar to the following:<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "Do you think you will observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance. Execute the following code cells to export the model in the ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.32564401626586914 seconds\n",
      "time of error check of onnx model:  9.962627649307251 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K May 31 19:44 .\n",
      "drwxr-xr-x 4 root root 4.0K May 31 19:44 ..\n",
      "drwxr-xr-x 2 root root 4.0K May 31 19:44 1\n",
      "-rw-r--r-- 1 root root  569 May 31 19:44 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Exercise: Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "!cp solutions/ex-2-1-3_config.pbtxt candidatemodels/bertQA-onnx-conexec/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 0.507642 infer/sec. p85 latency: 6270 usec\n",
      "  Pass [2] throughput: 162.918 infer/sec. p85 latency: 6195 usec\n",
      "  Pass [3] throughput: 162.923 infer/sec. p85 latency: 6182 usec\n",
      "  Pass [4] throughput: 162.904 infer/sec. p85 latency: 6195 usec\n",
      "  Client: \n",
      "    Request count: 489\n",
      "    Throughput: 162.915 infer/sec\n",
      "    Avg client overhead: 0.03%\n",
      "    Avg send request rate: 162.92 infer/sec\n",
      "    p50 latency: 6123 usec\n",
      "    p85 latency: 6192 usec\n",
      "    p90 latency: 6207 usec\n",
      "    p95 latency: 6234 usec\n",
      "    p99 latency: 6277 usec\n",
      "    Avg gRPC time: 6129 usec (marshal 2 usec + response wait 6126 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 489\n",
      "    Execution count: 489\n",
      "    Successful request count: 489\n",
      "    Avg request latency: 6001 usec (overhead 65 usec + queue 36 usec + compute input 29 usec + compute infer 5863 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 180.92 infer/sec. p85 latency: 11823 usec\n",
      "  Pass [2] throughput: 179.9 infer/sec. p85 latency: 11619 usec\n",
      "  Pass [3] throughput: 179.907 infer/sec. p85 latency: 11890 usec\n",
      "  Client: \n",
      "    Request count: 541\n",
      "    Throughput: 180.242 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 180.58 infer/sec\n",
      "    p50 latency: 11069 usec\n",
      "    p85 latency: 11768 usec\n",
      "    p90 latency: 12034 usec\n",
      "    p95 latency: 12345 usec\n",
      "    p99 latency: 13156 usec\n",
      "    Avg gRPC time: 11082 usec (marshal 2 usec + response wait 11079 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 541\n",
      "    Execution count: 541\n",
      "    Successful request count: 541\n",
      "    Avg request latency: 10919 usec (overhead 63 usec + queue 21 usec + compute input 52 usec + compute infer 10775 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 185.922 infer/sec. p85 latency: 18887 usec\n",
      "  Pass [2] throughput: 184.909 infer/sec. p85 latency: 19259 usec\n",
      "  Pass [3] throughput: 182.903 infer/sec. p85 latency: 18431 usec\n",
      "  Client: \n",
      "    Request count: 554\n",
      "    Throughput: 184.578 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 184.91 infer/sec\n",
      "    p50 latency: 16102 usec\n",
      "    p85 latency: 18914 usec\n",
      "    p90 latency: 19291 usec\n",
      "    p95 latency: 19627 usec\n",
      "    p99 latency: 20534 usec\n",
      "    Avg gRPC time: 16188 usec (marshal 2 usec + response wait 16185 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 554\n",
      "    Execution count: 554\n",
      "    Successful request count: 554\n",
      "    Avg request latency: 16016 usec (overhead 64 usec + queue 5215 usec + compute input 49 usec + compute infer 10679 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 184.929 infer/sec. p85 latency: 22220 usec\n",
      "  Pass [2] throughput: 185.883 infer/sec. p85 latency: 22120 usec\n",
      "  Pass [3] throughput: 184.905 infer/sec. p85 latency: 22389 usec\n",
      "  Client: \n",
      "    Request count: 556\n",
      "    Throughput: 185.239 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 186.24 infer/sec\n",
      "    p50 latency: 21587 usec\n",
      "    p85 latency: 22226 usec\n",
      "    p90 latency: 22429 usec\n",
      "    p95 latency: 22656 usec\n",
      "    p99 latency: 23136 usec\n",
      "    Avg gRPC time: 21540 usec (marshal 2 usec + response wait 21537 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 557\n",
      "    Execution count: 557\n",
      "    Successful request count: 557\n",
      "    Avg request latency: 21359 usec (overhead 65 usec + queue 10579 usec + compute input 42 usec + compute infer 10664 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 183.941 infer/sec. p85 latency: 29618 usec\n",
      "  Pass [2] throughput: 184.905 infer/sec. p85 latency: 29371 usec\n",
      "  Pass [3] throughput: 184.911 infer/sec. p85 latency: 29550 usec\n",
      "  Client: \n",
      "    Request count: 554\n",
      "    Throughput: 184.586 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 184.92 infer/sec\n",
      "    p50 latency: 27087 usec\n",
      "    p85 latency: 29518 usec\n",
      "    p90 latency: 29711 usec\n",
      "    p95 latency: 30167 usec\n",
      "    p99 latency: 30918 usec\n",
      "    Avg gRPC time: 27024 usec (marshal 2 usec + response wait 27021 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 554\n",
      "    Execution count: 554\n",
      "    Successful request count: 554\n",
      "    Avg request latency: 26830 usec (overhead 64 usec + queue 16014 usec + compute input 45 usec + compute infer 10699 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 184.933 infer/sec. p85 latency: 33520 usec\n",
      "  Pass [2] throughput: 184.897 infer/sec. p85 latency: 33552 usec\n",
      "  Pass [3] throughput: 184.9 infer/sec. p85 latency: 33489 usec\n",
      "  Client: \n",
      "    Request count: 555\n",
      "    Throughput: 184.91 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 185.24 infer/sec\n",
      "    p50 latency: 32368 usec\n",
      "    p85 latency: 33534 usec\n",
      "    p90 latency: 33712 usec\n",
      "    p95 latency: 34012 usec\n",
      "    p99 latency: 34777 usec\n",
      "    Avg gRPC time: 32337 usec (marshal 2 usec + response wait 32334 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 555\n",
      "    Execution count: 555\n",
      "    Successful request count: 555\n",
      "    Avg request latency: 32140 usec (overhead 63 usec + queue 21352 usec + compute input 44 usec + compute infer 10672 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 185.936 infer/sec. p85 latency: 40754 usec\n",
      "  Pass [2] throughput: 185.904 infer/sec. p85 latency: 40844 usec\n",
      "  Pass [3] throughput: 185.9 infer/sec. p85 latency: 40992 usec\n",
      "  Client: \n",
      "    Request count: 558\n",
      "    Throughput: 185.913 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 186.25 infer/sec\n",
      "    p50 latency: 37766 usec\n",
      "    p85 latency: 40881 usec\n",
      "    p90 latency: 41157 usec\n",
      "    p95 latency: 41468 usec\n",
      "    p99 latency: 42052 usec\n",
      "    Avg gRPC time: 37665 usec (marshal 2 usec + response wait 37662 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 558\n",
      "    Execution count: 558\n",
      "    Successful request count: 558\n",
      "    Avg request latency: 37460 usec (overhead 63 usec + queue 26688 usec + compute input 43 usec + compute infer 10657 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 183.927 infer/sec. p85 latency: 44301 usec\n",
      "  Pass [2] throughput: 183.891 infer/sec. p85 latency: 44427 usec\n",
      "  Pass [3] throughput: 185.892 infer/sec. p85 latency: 44247 usec\n",
      "  Client: \n",
      "    Request count: 554\n",
      "    Throughput: 184.57 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 184.90 infer/sec\n",
      "    p50 latency: 43452 usec\n",
      "    p85 latency: 44319 usec\n",
      "    p90 latency: 44558 usec\n",
      "    p95 latency: 44869 usec\n",
      "    p99 latency: 45850 usec\n",
      "    Avg gRPC time: 43263 usec (marshal 2 usec + response wait 43260 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 554\n",
      "    Execution count: 554\n",
      "    Successful request count: 554\n",
      "    Avg request latency: 43065 usec (overhead 61 usec + queue 32245 usec + compute input 45 usec + compute infer 10706 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 184.93 infer/sec. p85 latency: 51452 usec\n",
      "  Pass [2] throughput: 184.899 infer/sec. p85 latency: 51066 usec\n",
      "  Pass [3] throughput: 183.852 infer/sec. p85 latency: 50985 usec\n",
      "  Client: \n",
      "    Request count: 554\n",
      "    Throughput: 184.56 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 184.89 infer/sec\n",
      "    p50 latency: 48732 usec\n",
      "    p85 latency: 51160 usec\n",
      "    p90 latency: 51491 usec\n",
      "    p95 latency: 51827 usec\n",
      "    p99 latency: 52635 usec\n",
      "    Avg gRPC time: 48569 usec (marshal 2 usec + response wait 48566 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 554\n",
      "    Execution count: 554\n",
      "    Successful request count: 554\n",
      "    Avg request latency: 48366 usec (overhead 60 usec + queue 37565 usec + compute input 44 usec + compute infer 10688 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 184.925 infer/sec. p85 latency: 55326 usec\n",
      "  Pass [2] throughput: 185.886 infer/sec. p85 latency: 54929 usec\n",
      "  Pass [3] throughput: 184.887 infer/sec. p85 latency: 55358 usec\n",
      "  Client: \n",
      "    Request count: 556\n",
      "    Throughput: 185.233 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 185.90 infer/sec\n",
      "    p50 latency: 53991 usec\n",
      "    p85 latency: 55237 usec\n",
      "    p90 latency: 55613 usec\n",
      "    p95 latency: 56121 usec\n",
      "    p99 latency: 57186 usec\n",
      "    Avg gRPC time: 53918 usec (marshal 2 usec + response wait 53915 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 556\n",
      "    Execution count: 556\n",
      "    Successful request count: 556\n",
      "    Avg request latency: 53713 usec (overhead 61 usec + queue 42925 usec + compute input 46 usec + compute infer 10673 usec + compute output 8 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 162.915 infer/sec, latency 6192 usec\n",
      "Concurrency: 2, throughput: 180.242 infer/sec, latency 11768 usec\n",
      "Concurrency: 3, throughput: 184.578 infer/sec, latency 18914 usec\n",
      "Concurrency: 4, throughput: 185.239 infer/sec, latency 22226 usec\n",
      "Concurrency: 5, throughput: 184.586 infer/sec, latency 29518 usec\n",
      "Concurrency: 6, throughput: 184.91 infer/sec, latency 33534 usec\n",
      "Concurrency: 7, throughput: 185.913 infer/sec, latency 40881 usec\n",
      "Concurrency: 8, throughput: 184.57 infer/sec, latency 44319 usec\n",
      "Concurrency: 9, throughput: 184.56 infer/sec, latency 51160 usec\n",
      "Concurrency: 10, throughput: 185.233 infer/sec, latency 55237 usec\n",
      "CPU times: user 1.17 s, sys: 440 ms, total: 1.61 s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-conexec\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: inter-device move failed: '/dli/task/model_repository/bertQA-onnx-trt-fp16' to '/dli/task/candidatemodels/bertQA-onnx-trt-fp16'; unable to remove target: Directory not empty\n",
      "bertQA-onnx-trt-fp16  bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv -f /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances. We will discuss this particular option in the next section of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "The sequence batcher can employ one of two scheduling strategies when deciding how to batch the sequences that are routed to the same model instance. These strategies are Direct and Oldest.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated.\n",
    "\n",
    "With the Oldest scheduling strategy the sequence batcher ensures that all inference requests in a sequence are routed to the same model instance and then uses the dynamic batcher to batch together multiple inferences from different sequences into a batch that inferences together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. An example of an ensemble pipeline is illustrated below: <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view.\n",
    "\n",
    "More information on Triton scheduling can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#stateless-models\">following section of the documentation</a>. In this class, we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Exercise: Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  warnings.warn(\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.0135725736618042\n",
      "\n",
      "average L_inf error over output tensors:  0.008322536945343018\n",
      "variance of L_inf error over output tensors:  1.9936597752234775e-05\n",
      "stddev of L_inf error over output tensors:  0.0044650417413765325\n",
      "\n",
      "time of error check of native model:  0.32573747634887695 seconds\n",
      "time of error check of onnx model:  10.011567831039429 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "!cp solutions/ex-2-3-1_config.pbtxt candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 196.749 infer/sec\n",
      "    p50 latency: 40620 usec\n",
      "    p85 latency: 40680 usec\n",
      "    p90 latency: 40689 usec\n",
      "    p95 latency: 40704 usec\n",
      "    p99 latency: 40722 usec\n",
      "    Avg gRPC time: 40613 usec ((un)marshal request/response 6 usec + response wait 40607 usec)\n",
      "  Server: \n",
      "    Inference count: 984\n",
      "    Execution count: 123\n",
      "    Successful request count: 123\n",
      "    Avg request latency: 40388 usec (overhead 71 usec + queue 63 usec + compute input 30 usec + compute infer 40214 usec + compute output 9 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 196.749 infer/sec, latency 40680 usec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 4\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 126\n",
      "    Throughput: 167.925 infer/sec\n",
      "    p50 latency: 23810 usec\n",
      "    p85 latency: 23857 usec\n",
      "    p90 latency: 23885 usec\n",
      "    p95 latency: 23903 usec\n",
      "    p99 latency: 23952 usec\n",
      "    Avg gRPC time: 23805 usec ((un)marshal request/response 4 usec + response wait 23801 usec)\n",
      "  Server: \n",
      "    Inference count: 504\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 23618 usec (overhead 72 usec + queue 58 usec + compute input 28 usec + compute infer 23451 usec + compute output 9 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 167.925 infer/sec, latency 23857 usec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 25\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 263\n",
      "    Throughput: 87.6113 infer/sec\n",
      "    p50 latency: 11384 usec\n",
      "    p85 latency: 11521 usec\n",
      "    p90 latency: 11561 usec\n",
      "    p95 latency: 11619 usec\n",
      "    p99 latency: 11665 usec\n",
      "    Avg gRPC time: 11406 usec ((un)marshal request/response 4 usec + response wait 11402 usec)\n",
      "  Server: \n",
      "    Inference count: 263\n",
      "    Execution count: 263\n",
      "    Successful request count: 263\n",
      "    Avg request latency: 11223 usec (overhead 69 usec + queue 191 usec + compute input 26 usec + compute infer 10928 usec + compute output 8 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 87.6113 infer/sec, latency 11521 usec\n",
      "CPU times: user 2.03 s, sys: 756 ms, total: 2.79 s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# warm up model with some inferences for faster analysis  (takes about 5 minutes)\n",
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "batchSize = 8\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}\n",
    "batchSize = 4\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}\n",
    "batchSize = 1\n",
    "!./utilities/run_warmup.sh {modelName} {batchSize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"count_windows\" mode for stabilization\n",
      "  Minimum number of samples in each window: 50\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using p85 latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 86.965 infer/sec. p85 latency: 11441 usec\n",
      "  Pass [2] throughput: 87.933 infer/sec. p85 latency: 11395 usec\n",
      "  Pass [3] throughput: 87.9521 infer/sec. p85 latency: 11396 usec\n",
      "  Client: \n",
      "    Request count: 263\n",
      "    Throughput: 87.6167 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 87.95 infer/sec\n",
      "    p50 latency: 11358 usec\n",
      "    p85 latency: 11410 usec\n",
      "    p90 latency: 11426 usec\n",
      "    p95 latency: 11448 usec\n",
      "    p99 latency: 11541 usec\n",
      "    Avg gRPC time: 11364 usec (marshal 2 usec + response wait 11361 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 263\n",
      "    Execution count: 263\n",
      "    Successful request count: 263\n",
      "    Avg request latency: 11233 usec (overhead 71 usec + queue 193 usec + compute input 25 usec + compute infer 10936 usec + compute output 8 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 89.967 infer/sec. p85 latency: 22122 usec\n",
      "  Pass [2] throughput: 90.9592 infer/sec. p85 latency: 22130 usec\n",
      "  Pass [3] throughput: 89.9577 infer/sec. p85 latency: 22138 usec\n",
      "  Client: \n",
      "    Request count: 271\n",
      "    Throughput: 90.2946 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 90.96 infer/sec\n",
      "    p50 latency: 22093 usec\n",
      "    p85 latency: 22130 usec\n",
      "    p90 latency: 22141 usec\n",
      "    p95 latency: 22155 usec\n",
      "    p99 latency: 22184 usec\n",
      "    Avg gRPC time: 22047 usec (marshal 2 usec + response wait 22044 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 271\n",
      "    Execution count: 271\n",
      "    Successful request count: 271\n",
      "    Avg request latency: 21901 usec (overhead 67 usec + queue 10875 usec + compute input 23 usec + compute infer 10928 usec + compute output 7 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 111.947 infer/sec. p85 latency: 26548 usec\n",
      "  Pass [2] throughput: 112.93 infer/sec. p85 latency: 26523 usec\n",
      "  Pass [3] throughput: 113.953 infer/sec. p85 latency: 26527 usec\n",
      "  Client: \n",
      "    Request count: 339\n",
      "    Throughput: 112.943 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 113.28 infer/sec\n",
      "    p50 latency: 26482 usec\n",
      "    p85 latency: 26532 usec\n",
      "    p90 latency: 26548 usec\n",
      "    p95 latency: 26576 usec\n",
      "    p99 latency: 37358 usec\n",
      "    Avg gRPC time: 26537 usec (marshal 3 usec + response wait 26533 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 339\n",
      "    Execution count: 228\n",
      "    Successful request count: 339\n",
      "    Avg request latency: 26320 usec (overhead 88 usec + queue 12459 usec + compute input 33 usec + compute infer 13728 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 129.956 infer/sec. p85 latency: 30559 usec\n",
      "  Pass [2] throughput: 128.944 infer/sec. p85 latency: 30649 usec\n",
      "  Pass [3] throughput: 127.941 infer/sec. p85 latency: 30629 usec\n",
      "  Client: \n",
      "    Request count: 387\n",
      "    Throughput: 128.947 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 129.28 infer/sec\n",
      "    p50 latency: 30516 usec\n",
      "    p85 latency: 30599 usec\n",
      "    p90 latency: 30661 usec\n",
      "    p95 latency: 41587 usec\n",
      "    p99 latency: 41757 usec\n",
      "    Avg gRPC time: 30866 usec (marshal 3 usec + response wait 30862 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 387\n",
      "    Execution count: 199\n",
      "    Successful request count: 387\n",
      "    Avg request latency: 30647 usec (overhead 113 usec + queue 15485 usec + compute input 41 usec + compute infer 14997 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 136.949 infer/sec. p85 latency: 45711 usec\n",
      "  Pass [2] throughput: 139.9 infer/sec. p85 latency: 35322 usec\n",
      "  Pass [3] throughput: 137.933 infer/sec. p85 latency: 35313 usec\n",
      "  Client: \n",
      "    Request count: 415\n",
      "    Throughput: 138.261 infer/sec\n",
      "    Avg client overhead: 0.02%\n",
      "    Avg send request rate: 138.59 infer/sec\n",
      "    p50 latency: 35212 usec\n",
      "    p85 latency: 35367 usec\n",
      "    p90 latency: 45749 usec\n",
      "    p95 latency: 46223 usec\n",
      "    p99 latency: 50322 usec\n",
      "    Avg gRPC time: 36050 usec (marshal 3 usec + response wait 36046 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 415\n",
      "    Execution count: 178\n",
      "    Successful request count: 415\n",
      "    Avg request latency: 35833 usec (overhead 131 usec + queue 18461 usec + compute input 47 usec + compute infer 17181 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 149.943 infer/sec. p85 latency: 44288 usec\n",
      "  Pass [2] throughput: 143.935 infer/sec. p85 latency: 50416 usec\n",
      "  Pass [3] throughput: 145.922 infer/sec. p85 latency: 50399 usec\n",
      "  Client: \n",
      "    Request count: 440\n",
      "    Throughput: 146.6 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 146.93 infer/sec\n",
      "    p50 latency: 39399 usec\n",
      "    p85 latency: 50379 usec\n",
      "    p90 latency: 50482 usec\n",
      "    p95 latency: 54503 usec\n",
      "    p99 latency: 59526 usec\n",
      "    Avg gRPC time: 41067 usec (marshal 3 usec + response wait 41063 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 440\n",
      "    Execution count: 163\n",
      "    Successful request count: 440\n",
      "    Avg request latency: 40818 usec (overhead 136 usec + queue 21099 usec + compute input 43 usec + compute infer 19525 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 146.949 infer/sec. p85 latency: 54906 usec\n",
      "  Pass [2] throughput: 148.923 infer/sec. p85 latency: 48820 usec\n",
      "  Pass [3] throughput: 152.934 infer/sec. p85 latency: 48436 usec\n",
      "  Client: \n",
      "    Request count: 449\n",
      "    Throughput: 149.602 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 149.94 infer/sec\n",
      "    p50 latency: 48156 usec\n",
      "    p85 latency: 48830 usec\n",
      "    p90 latency: 54976 usec\n",
      "    p95 latency: 63406 usec\n",
      "    p99 latency: 68123 usec\n",
      "    Avg gRPC time: 46607 usec (marshal 3 usec + response wait 46603 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 449\n",
      "    Execution count: 138\n",
      "    Successful request count: 449\n",
      "    Avg request latency: 46362 usec (overhead 152 usec + queue 22085 usec + compute input 47 usec + compute infer 24062 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 149.942 infer/sec. p85 latency: 59089 usec\n",
      "  Pass [2] throughput: 154.918 infer/sec. p85 latency: 53071 usec\n",
      "  Pass [3] throughput: 152.935 infer/sec. p85 latency: 53132 usec\n",
      "  Client: \n",
      "    Request count: 458\n",
      "    Throughput: 152.598 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 152.93 infer/sec\n",
      "    p50 latency: 52883 usec\n",
      "    p85 latency: 53135 usec\n",
      "    p90 latency: 62932 usec\n",
      "    p95 latency: 72693 usec\n",
      "    p99 latency: 76822 usec\n",
      "    Avg gRPC time: 52452 usec (marshal 3 usec + response wait 52448 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 458\n",
      "    Execution count: 122\n",
      "    Successful request count: 458\n",
      "    Avg request latency: 52195 usec (overhead 169 usec + queue 24263 usec + compute input 50 usec + compute infer 27694 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 151.914 infer/sec. p85 latency: 68336 usec\n",
      "  Pass [2] throughput: 155.895 infer/sec. p85 latency: 57537 usec\n",
      "  Pass [3] throughput: 162.895 infer/sec. p85 latency: 67527 usec\n",
      "  Client: \n",
      "    Request count: 471\n",
      "    Throughput: 156.902 infer/sec\n",
      "    Avg client overhead: 0.01%\n",
      "    Avg send request rate: 157.23 infer/sec\n",
      "    p50 latency: 57067 usec\n",
      "    p85 latency: 57678 usec\n",
      "    p90 latency: 72607 usec\n",
      "    p95 latency: 80584 usec\n",
      "    p99 latency: 90313 usec\n",
      "    Avg gRPC time: 57465 usec (marshal 3 usec + response wait 57461 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 471\n",
      "    Execution count: 112\n",
      "    Successful request count: 471\n",
      "    Avg request latency: 57200 usec (overhead 187 usec + queue 27178 usec + compute input 55 usec + compute infer 29761 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 158.941 infer/sec. p85 latency: 65850 usec\n",
      "  Pass [2] throughput: 160.923 infer/sec. p85 latency: 65772 usec\n",
      "  Pass [3] throughput: 157.926 infer/sec. p85 latency: 65875 usec\n",
      "  Client: \n",
      "    Request count: 478\n",
      "    Throughput: 159.264 infer/sec\n",
      "    Avg client overhead: 0.00%\n",
      "    Avg send request rate: 159.60 infer/sec\n",
      "    p50 latency: 61596 usec\n",
      "    p85 latency: 65823 usec\n",
      "    p90 latency: 66136 usec\n",
      "    p95 latency: 85190 usec\n",
      "    p99 latency: 94021 usec\n",
      "    Avg gRPC time: 62182 usec (marshal 3 usec + response wait 62178 usec + unmarshal 1 usec)\n",
      "  Server: \n",
      "    Inference count: 478\n",
      "    Execution count: 101\n",
      "    Successful request count: 478\n",
      "    Avg request latency: 61901 usec (overhead 177 usec + queue 28601 usec + compute input 47 usec + compute infer 33058 usec + compute output 18 usec)\n",
      "\n",
      "Inferences/Second vs. Client p85 Batch Latency\n",
      "Concurrency: 1, throughput: 87.6167 infer/sec, latency 11410 usec\n",
      "Concurrency: 2, throughput: 90.2946 infer/sec, latency 22130 usec\n",
      "Concurrency: 3, throughput: 112.943 infer/sec, latency 26532 usec\n",
      "Concurrency: 4, throughput: 128.947 infer/sec, latency 30599 usec\n",
      "Concurrency: 5, throughput: 138.261 infer/sec, latency 35367 usec\n",
      "Concurrency: 6, throughput: 146.6 infer/sec, latency 50379 usec\n",
      "Concurrency: 7, throughput: 149.602 infer/sec, latency 48830 usec\n",
      "Concurrency: 8, throughput: 152.598 infer/sec, latency 53135 usec\n",
      "Concurrency: 9, throughput: 156.902 infer/sec, latency 57678 usec\n",
      "Concurrency: 10, throughput: 159.264 infer/sec, latency 65823 usec\n",
      "CPU times: user 315 ms, sys: 122 ms, total: 437 ms\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "\n",
    "!./utilities/run_perf_analyzer_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData} \\\n",
    "                    {measurement_request_count} \\\n",
    "                    {percentile_stability} \\\n",
    "                    {stability_percentage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have observed a fairly dramatic improvement in both latency and throughput. \n",
    "* How big is the impact in comparison to vanilla ONNX configuration or vanilla TorchScript? \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n",
    "\n",
    "Discuss the results with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've leaned some strategies to improve the GPU utilization and reduce latency using:\n",
    "\n",
    "* Concurrent model execution\n",
    "* Scheduling\n",
    "* Dynamic batching\n",
    "\n",
    "In the next segment of the class we will make a more formal assessment of inference performance across multiple concurrency levels and how to analyze your inference performance in a structured way. Please proceed to the next notebook:<br>\n",
    "[3.0 Server Performance](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
